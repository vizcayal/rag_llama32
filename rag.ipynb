{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "local_llm = 'llama3.2:3b-instruct-fp16'\n",
    "llm = ChatOllama(model = local_llm, temperature = 0)\n",
    "llm_json_mode = ChatOllama(model = local_llm, temperature = 0, format = 'json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var:str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}:\")\n",
    "\n",
    "_set_env('TAVILY_API_KEY')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] =  \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# load docs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# split docs\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 1000, chunk_overlap = 200\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents = doc_splits,\n",
    "    embedding = NomicEmbeddings(model = 'nomic-embed-text-v1.5', inference_mode='local')\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'websearch'} {'datasource': 'websearch'} {'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "#prompt\n",
    "\n",
    "router_instructions = '''You are an expert at routing a user question to a vectorstore or web search\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "use the vectorstore for questions on these topicas. For all else, and specially for current events, use web-search.\n",
    "\n",
    "return Json iwth single key, datasource, that is 'websearch' or 'vectorstore' depending on the question\n",
    "'''\n",
    "\n",
    "# Test router\n",
    "test_web_search = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"Who is favored to win the NFC Championship game in the 2024 season?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "test_web_search_2 = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [HumanMessage(content=\"What are the models released today for llama3.2?\")]\n",
    ")\n",
    "test_vector_store = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [HumanMessage(content=\"What are the types of agent memory?\")]\n",
    ")\n",
    "print(\n",
    "    json.loads(test_web_search.content),\n",
    "    json.loads(test_web_search_2.content),\n",
    "    json.loads(test_vector_store.content),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieval grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc grader instructions\n",
    "\n",
    "doc_grader_intructions = '''\n",
    "you are a grader assessing relevance of a retrieved document to a user question.\n",
    "if the document contains keyword(s) or semantic meaning related to the question, \n",
    "grade it as relevant\n",
    "'''\n",
    "\n",
    "#grader prompt\n",
    "doc_grader_prompt = \"\"\"\n",
    "here is the retrieved document: \\n\\n {document}\n",
    "here is the user question: \\n\\n {question}.\n",
    "\n",
    "then carefully and objectively assess whether the document contains at least \n",
    "some information that is relevant to the question.\n",
    "\n",
    "return json with single key, binary_score, that is 'yes' or 'no' score to indicate \n",
    "whether the document contains at least some information that is relevant to the question\n",
    "\"\"\"\n",
    "\n",
    "# test\n",
    "\n",
    "question = 'What is chain of thought prompting?'\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(document = doc_text, question = question )\n",
    "\n",
    "result = llm_json_mode.invoke(\n",
    "[SystemMessage(content = doc_grader_intructions)] +\n",
    "[HumanMessage(content= doc_grader_prompt_formatted)]\n",
    ")\n",
    "\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-thought prompting is a technique used in natural language processing (NLP) where a model generates a sequence of reasoning steps, or \"thoughts\", to arrive at a desired output. This approach involves using external search queries, such as Wikipedia APIs, to retrieve relevant information and then incorporating it into the context. Chain-of-thought prompting can be further extended by exploring multiple reasoning possibilities at each step, generating multiple thoughts per step, and evaluating each state with a classifier or majority vote.\n"
     ]
    }
   ],
   "source": [
    "rag_prompt = \"\"\"\n",
    "You are an assistant for quertion-answering takss.\n",
    "here is the context to use to answer the question:\n",
    "{context}\n",
    "think carefully about the above context.\n",
    "\n",
    "Now, review the user question:\n",
    "{question}\n",
    "\n",
    "provide an answer to this questions using only the above context.\n",
    "\n",
    "use three sentences maximum and keep the answer concise\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context = docs_txt, question = question)\n",
    "generation = llm.invoke([HumanMessage(content = rag_prompt_formatted)])\n",
    "print(generation.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
